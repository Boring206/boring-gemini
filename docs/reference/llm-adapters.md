# LLM é©é…å™¨é–‹ç™¼æŒ‡å— (LLM Adapter Development Guide)

> **è§£æ±ºé¢¨éšª**: å°ç‰¹å®šæŠ€è¡“æ£§ (Gemini) çš„å¼·ä¾è³´

æœ¬æŒ‡å—èªªæ˜å¦‚ä½•ç‚º Boring-Gemini æ·»åŠ æ–°çš„ LLM Providerï¼Œå¯¦ç¾æŠ€è¡“æ£§å¤šå…ƒåŒ–ã€‚

---

## ğŸ¯ æ¦‚è¿°

Boring-Gemini ä½¿ç”¨æŠ½è±¡çš„ `LLMProvider` ä»‹é¢ä¾†æ”¯æŒå¤šç¨®èªè¨€æ¨¡å‹å¾Œç«¯ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LLM Provider æ¶æ§‹                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚                        â”‚   LLMProvider   â”‚                                  â”‚
â”‚                        â”‚    (Abstract)   â”‚                                  â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                                 â”‚                                            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚     â–¼               â–¼           â–¼           â–¼               â–¼               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚ â”‚  Gemini   â”‚ â”‚  Ollama   â”‚ â”‚  OpenAI   â”‚ â”‚  Claude   â”‚ â”‚   ä½ çš„    â”‚      â”‚
â”‚ â”‚  Provider â”‚ â”‚  Provider â”‚ â”‚  Compat   â”‚ â”‚  Adapter  â”‚ â”‚  Provider â”‚      â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ æ ¸å¿ƒä»‹é¢

### LLMProvider (æŠ½è±¡åŸºé¡)

ä½ç½®: `src/boring/llm/provider.py`

```python
from abc import abstractmethod
from boring.interfaces import LLMClient, LLMResponse


class LLMProvider(LLMClient):
    """
    Extended LLM Client interface that allows for more flexible configuration
    and swapping of backends (Gemini, Ollama, LMStudio, etc.)
    """

    @property
    @abstractmethod
    def model_name(self) -> str:
        """Name of the specific model being used"""
        pass

    @property
    @abstractmethod
    def is_available(self) -> bool:
        """Check if the provider/CLI is available and configured"""
        pass

    @abstractmethod
    def generate(
        self,
        prompt: str,
        context: str = "",
        system_instruction: str = "",
        timeout_seconds: int = 600,
    ) -> tuple[str, bool]:
        """
        Generate text from prompt and context.
        
        Returns:
            tuple[str, bool]: (generated_text, success)
        """
        pass

    @abstractmethod
    def generate_with_tools(
        self,
        prompt: str,
        context: str = "",
        system_instruction: str = "",
        timeout_seconds: int = 600,
    ) -> LLMResponse:
        """
        Generate text and/or function calls.
        
        Returns:
            LLMResponse with text and function_calls
        """
        pass

    def get_token_usage(self) -> dict[str, int]:
        """Return token usage statistics if available"""
        return {}
```

### LLMResponse (æ•¸æ“šæ¨¡å‹)

ä½ç½®: `src/boring/interfaces.py`

```python
from dataclasses import dataclass, field
from typing import Any


@dataclass
class LLMResponse:
    """Standardized LLM response"""
    text: str = ""
    function_calls: list[dict[str, Any]] = field(default_factory=list)
    success: bool = True
    error: str = ""
    raw_response: Any = None
```

---

## ğŸš€ å¯¦ç¾æ–°çš„ Provider

### æ­¥é©Ÿ 1: å‰µå»º Provider æ–‡ä»¶

åœ¨ `src/boring/llm/` å‰µå»ºæ–°æ–‡ä»¶ï¼Œä¾‹å¦‚ `my_provider.py`:

```python
"""
My Custom LLM Provider Implementation
"""

from pathlib import Path
from typing import Optional

from ..logger import get_logger
from .provider import LLMProvider, LLMResponse

_logger = get_logger("my_provider")


class MyProvider(LLMProvider):
    """
    Provider for My Custom LLM Service.
    """

    def __init__(
        self,
        model_name: str = "my-model-v1",
        api_key: Optional[str] = None,
        base_url: str = "https://api.my-llm.com",
        log_dir: Optional[Path] = None,
    ):
        self._model_name = model_name
        self.api_key = api_key
        self.base_url = base_url.rstrip("/")
        self.log_dir = log_dir or Path("logs")
        
        # Initialize your client here
        self._client = None
        if self.api_key:
            self._client = self._initialize_client()

    def _initialize_client(self):
        """Initialize the API client"""
        # Your initialization logic
        pass

    @property
    def model_name(self) -> str:
        return self._model_name

    @property
    def provider_name(self) -> str:
        """Optional: Provider identifier"""
        return "my_provider"

    @property
    def is_available(self) -> bool:
        """Check if the provider is available and configured"""
        if not self.api_key:
            return False
        try:
            # Perform a health check
            # e.g., ping the API
            return True
        except Exception:
            return False

    def generate(
        self,
        prompt: str,
        context: str = "",
        system_instruction: str = "",
        timeout_seconds: int = 600,
    ) -> tuple[str, bool]:
        """
        Generate text using the LLM.
        
        Args:
            prompt: The user's prompt
            context: Additional context (code, documentation, etc.)
            system_instruction: System-level instructions
            timeout_seconds: Request timeout
            
        Returns:
            tuple[str, bool]: (generated_text, success)
        """
        if not self.is_available:
            return "Error: Provider not available", False

        try:
            # Build the full prompt
            full_prompt = f"{context}\n\n{prompt}" if context else prompt
            
            # Make the API call
            response = self._call_api(full_prompt, system_instruction, timeout_seconds)
            
            return response, True

        except Exception as e:
            _logger.error(f"Generation failed: {e}")
            return str(e), False

    def generate_with_tools(
        self,
        prompt: str,
        context: str = "",
        system_instruction: str = "",
        timeout_seconds: int = 600,
    ) -> LLMResponse:
        """
        Generate text with function calling support.
        
        If your provider doesn't support native function calling,
        you can parse tool calls from the text response.
        """
        text, success = self.generate(prompt, context, system_instruction, timeout_seconds)
        
        # If your provider supports function calling natively:
        # function_calls = self._extract_function_calls(raw_response)
        
        return LLMResponse(
            text=text,
            function_calls=[],  # Populate if supported
            success=success,
        )

    def _call_api(
        self, prompt: str, system_instruction: str, timeout: int
    ) -> str:
        """Make the actual API call"""
        import requests
        
        response = requests.post(
            f"{self.base_url}/v1/chat/completions",
            headers={"Authorization": f"Bearer {self.api_key}"},
            json={
                "model": self.model_name,
                "messages": [
                    {"role": "system", "content": system_instruction},
                    {"role": "user", "content": prompt},
                ],
            },
            timeout=timeout,
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]

    def get_token_usage(self) -> dict[str, int]:
        """Return token usage if tracked"""
        return {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0,
        }
```

### æ­¥é©Ÿ 2: è¨»å†Š Provider

åœ¨ `src/boring/llm/__init__.py` ä¸­å°å‡º:

```python
from .my_provider import MyProvider

__all__ = [
    "LLMProvider",
    "GeminiProvider",
    "OllamaProvider",
    "MyProvider",  # æ–°å¢
]
```

### æ­¥é©Ÿ 3: æ·»åŠ é…ç½®æ”¯æŒ

åœ¨ `src/boring/config.py` æ·»åŠ é…ç½®:

```python
# æ–° Provider è¨­ç½®
MY_PROVIDER_API_KEY: Optional[str] = os.getenv("MY_PROVIDER_API_KEY")
MY_PROVIDER_MODEL: str = os.getenv("MY_PROVIDER_MODEL", "my-model-v1")
MY_PROVIDER_BASE_URL: str = os.getenv(
    "MY_PROVIDER_BASE_URL", "https://api.my-llm.com"
)
```

### æ­¥é©Ÿ 4: æ·»åŠ  Provider é¸æ“‡é‚è¼¯

åœ¨éœ€è¦ä½¿ç”¨ LLM çš„åœ°æ–¹æ·»åŠ é¸æ“‡é‚è¼¯:

```python
from boring.config import settings
from boring.llm import GeminiProvider, OllamaProvider, MyProvider


def get_llm_provider() -> LLMProvider:
    """Get the configured LLM provider with fallback chain"""
    
    # Priority: Gemini > MyProvider > Ollama (local)
    providers = [
        lambda: GeminiProvider() if settings.GOOGLE_API_KEY else None,
        lambda: MyProvider() if settings.MY_PROVIDER_API_KEY else None,
        lambda: OllamaProvider("llama3.2") if OllamaProvider("llama3.2").is_available else None,
    ]
    
    for get_provider in providers:
        provider = get_provider()
        if provider and provider.is_available:
            return provider
    
    raise RuntimeError("No LLM provider available")
```

---

## ğŸ§ª æ¸¬è©¦ä½ çš„ Provider

### å–®å…ƒæ¸¬è©¦

å‰µå»º `tests/unit/llm/test_my_provider.py`:

```python
import pytest
from unittest.mock import Mock, patch

from boring.llm.my_provider import MyProvider


class TestMyProvider:
    """Tests for MyProvider"""

    def test_initialization(self):
        """Test provider initialization"""
        provider = MyProvider(
            model_name="test-model",
            api_key="test-key",
        )
        assert provider.model_name == "test-model"
        assert provider.api_key == "test-key"

    def test_is_available_without_key(self):
        """Test availability check without API key"""
        provider = MyProvider(api_key=None)
        assert provider.is_available is False

    @patch("boring.llm.my_provider.requests.post")
    def test_generate_success(self, mock_post):
        """Test successful generation"""
        mock_post.return_value.status_code = 200
        mock_post.return_value.json.return_value = {
            "choices": [{"message": {"content": "Hello, World!"}}]
        }

        provider = MyProvider(api_key="test-key")
        text, success = provider.generate("Say hello")

        assert success is True
        assert "Hello" in text

    @patch("boring.llm.my_provider.requests.post")
    def test_generate_failure(self, mock_post):
        """Test generation failure handling"""
        mock_post.side_effect = Exception("API Error")

        provider = MyProvider(api_key="test-key")
        text, success = provider.generate("Say hello")

        assert success is False
        assert "Error" in text or "API Error" in text
```

### æ•´åˆæ¸¬è©¦

å‰µå»º `tests/integration/test_my_provider_integration.py`:

```python
import os
import pytest

from boring.llm.my_provider import MyProvider


@pytest.mark.integration
@pytest.mark.skipif(
    not os.getenv("MY_PROVIDER_API_KEY"),
    reason="MY_PROVIDER_API_KEY not set"
)
class TestMyProviderIntegration:
    """Integration tests for MyProvider (requires real API key)"""

    def test_real_generation(self):
        """Test real API call"""
        provider = MyProvider(api_key=os.getenv("MY_PROVIDER_API_KEY"))
        
        text, success = provider.generate("What is 2+2? Reply with just the number.")
        
        assert success is True
        assert "4" in text
```

---

## ğŸ“‹ å¯¦ç¾æª¢æŸ¥æ¸…å–®

åœ¨æäº¤ PR å‰ç¢ºèª:

- [ ] å¯¦ç¾äº† `LLMProvider` çš„æ‰€æœ‰æŠ½è±¡æ–¹æ³•
- [ ] è™•ç†äº† API éŒ¯èª¤å’Œè¶…æ™‚
- [ ] æ·»åŠ äº†é©ç•¶çš„æ—¥èªŒè¨˜éŒ„
- [ ] é…ç½®é€šéç’°å¢ƒè®Šé‡è®€å–
- [ ] æœ‰ `is_available` å¥åº·æª¢æŸ¥
- [ ] ç·¨å¯«äº†å–®å…ƒæ¸¬è©¦ (â‰¥80% è¦†è“‹)
- [ ] ç·¨å¯«äº†æ•´åˆæ¸¬è©¦ (å¯é¸ï¼Œéœ€çœŸå¯¦ API)
- [ ] æ›´æ–°äº† `docs/reference/feature-matrix.md`
- [ ] æ›´æ–°äº† `pyproject.toml` æ·»åŠ å¯é¸ä¾è³´ (å¦‚éœ€)
- [ ] æ›´æ–°äº† README èªªæ˜æ–° Provider

---

## ğŸ”„ åŠŸèƒ½é™ç´šç­–ç•¥

### é™ç´šéˆ

```
Gemini (é›²ç«¯) â†’ Ollama (æœ¬åœ°) â†’ åŠŸèƒ½ç¦ç”¨
     â”‚              â”‚              â”‚
     â–¼              â–¼              â–¼
  API Key       æœ¬åœ°é‹è¡Œ        éŒ¯èª¤æç¤º
```

### å¯¦ç¾é™ç´š

```python
def generate_with_fallback(prompt: str) -> tuple[str, bool]:
    """Generate with automatic fallback to available providers"""
    
    providers = [
        ("gemini", lambda: GeminiProvider()),
        ("ollama", lambda: OllamaProvider("llama3.2")),
    ]
    
    for name, get_provider in providers:
        try:
            provider = get_provider()
            if provider.is_available:
                return provider.generate(prompt)
        except Exception as e:
            _logger.warning(f"Provider {name} failed: {e}")
            continue
    
    return "Error: All LLM providers unavailable", False
```

---

## ğŸ·ï¸ ç¾æœ‰ Provider åƒè€ƒ

| Provider | æ–‡ä»¶ | ç‰¹é» |
|----------|------|------|
| `GeminiProvider` | `gemini.py` | SDK + CLI é›™æ¨¡å¼ |
| `OllamaProvider` | `ollama.py` | æœ¬åœ°é‹è¡Œï¼Œç„¡éœ€ API Key |
| `OpenAICompatProvider` | `openai_compat.py` | é€šç”¨ OpenAI å…¼å®¹ API |
| `ClaudeAdapter` | `claude_adapter.py` | Anthropic Claude |

---

## ğŸ¤ è²¢ç»

å¦‚æœä½ å¯¦ç¾äº†æ–°çš„ Providerï¼Œæ­¡è¿æäº¤ PRï¼

è«‹ç¢ºä¿:
1. éµå¾ªæœ¬æŒ‡å—çš„è¦ç¯„
2. é€šéæ‰€æœ‰æ¸¬è©¦
3. æ›´æ–°æ–‡æª”

è©³è¦‹- [Contributing Guide](./contributing.md)

---

*æœ€å¾Œæ›´æ–°: 2026-01-12 | ç‰ˆæœ¬: 1.0.0*
