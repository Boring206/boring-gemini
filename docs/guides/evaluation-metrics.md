# è©•ä¼°æŒ‡æ¨™æŒ‡å— (Evaluation Metrics Guide)

boring-gemini V10.25 æ–°å¢çš„ LLM-as-a-Judge è©•ä¼°ç³»çµ±ï¼Œæä¾›å®Œæ•´çš„çµ±è¨ˆæŒ‡æ¨™ä¾†é©—è­‰è©•ä¼°å“è³ªã€‚

---

## ğŸ“Š æ ¸å¿ƒæŒ‡æ¨™ç¸½è¦½

| æŒ‡æ¨™ | æ¸¬é‡ä»€éº¼ | é©ç”¨å ´æ™¯ | ç¯„åœ |
|------|----------|----------|------|
| **Cohen's Îº (Kappa)** | å…©å€‹è©•å¯©è€…çš„**ä¸€è‡´æ€§** | AI è©•åˆ† vs äººé¡è©•å¯© | -1 ~ 1 |
| **Spearman's Ï (Rho)** | å…©çµ„æ’åçš„**ç›¸é—œæ€§** | åˆ†æ•¸çš„æ’åºæ˜¯å¦ä¸€è‡´ | -1 ~ 1 |
| **F1 Score** | åˆ†é¡çš„**æº–ç¢ºåº¦** | é€šé/ä¸é€šé ç­‰äºŒåˆ†é¡ | 0 ~ 1 |
| **Position Consistency** | æˆå°æ¯”è¼ƒçš„**ç©©å®šæ€§** | A vs B æ¯”è¼ƒæ˜¯å¦æœ‰åè¦‹ | 0 ~ 1 |

---

## ğŸ¯ è©³ç´°èªªæ˜

### 1ï¸âƒ£ Cohen's Kappa (ä¸€è‡´æ€§æŒ‡æ¨™)

**å•é¡Œ**ï¼šã€ŒAI è©•åˆ†è·Ÿäººé¡å°ˆå®¶ä¸€è‡´å—ï¼Ÿã€

```python
from boring.judge.metrics import cohens_kappa

human_scores = [4, 3, 5, 2, 4]
ai_scores = [4, 3, 4, 2, 4]  # ç¬¬3å€‹ä¸åŒ (5 vs 4)

kappa = cohens_kappa(ai_scores, human_scores)
print(f"Kappa: {kappa:.2f}")  # 0.71 - é«˜åº¦ä¸€è‡´
```

**è§£è®€æ¨™æº–**ï¼š

| Îº å€¼ | è§£è®€ |
|------|------|
| > 0.8 | å¹¾ä¹å®Œç¾ä¸€è‡´ |
| 0.6-0.8 | **é«˜åº¦ä¸€è‡´** âœ… |
| 0.4-0.6 | ä¸­ç­‰ä¸€è‡´ |
| 0.2-0.4 | ä¸€èˆ¬ä¸€è‡´ |
| < 0.2 | å¾®å¼±ä¸€è‡´ |

**ç”¨é€”**ï¼šé©—è­‰ AI è©•ä¼°ç³»çµ±æ˜¯å¦å¯ä»¥**å–ä»£äººé¡å¯©æŸ¥**

---

### 2ï¸âƒ£ Spearman's Ï (ç›¸é—œæ€§æŒ‡æ¨™)

**å•é¡Œ**ï¼šã€ŒAI æ’åé †åºè·Ÿäººé¡ä¸€æ¨£å—ï¼Ÿã€

```python
from boring.judge.metrics import spearmans_rho

human_ranks = [1, 2, 3, 4, 5]
ai_ranks = [1, 2, 3, 4, 5]  # æ’åå®Œå…¨ä¸€è‡´

rho, p_value = spearmans_rho(ai_ranks, human_ranks)
print(f"Spearman Ï: {rho:.2f}")  # 1.0 - å®Œç¾ç›¸é—œ
```

**è§£è®€æ¨™æº–**ï¼š

| Ï å€¼ | è§£è®€ |
|------|------|
| > 0.9 | **å¼·ç›¸é—œ** âœ… |
| 0.7-0.9 | ä¸­ç›¸é—œ |
| 0.5-0.7 | å¼±ç›¸é—œ |
| < 0.5 | ç„¡é¡¯è‘—ç›¸é—œ |

**ç”¨é€”**ï¼šå³ä½¿åˆ†æ•¸æ•¸å€¼ä¸åŒï¼Œé©—è­‰**æ’åºæ˜¯å¦æ­£ç¢º**

> [!TIP]
> Spearman é©åˆåºæ•¸è³‡æ–™ï¼ˆå¦‚ 1-5 åˆ†è©•åˆ†ï¼‰ï¼Œå› ç‚ºå®ƒåªçœ‹æ’åé †åºï¼Œä¸å—åˆ†æ•¸çµ•å°å€¼å½±éŸ¿ã€‚

---

### 3ï¸âƒ£ F1 Score (åˆ†é¡æº–ç¢ºåº¦)

**å•é¡Œ**ï¼šã€ŒAI åˆ¤æ–·é€šé/ä¸é€šéæº–ç¢ºå—ï¼Ÿã€

```python
from boring.judge.metrics import f1_score

actual = [1, 1, 0, 1]    # 1=é€šé, 0=ä¸é€šé
predicted = [1, 0, 0, 1]  # AI é æ¸¬

f1 = f1_score(predicted, actual)
print(f"F1: {f1:.2f}")  # 0.80
```

**å…¬å¼**ï¼š

```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

**ç”¨é€”**ï¼šè©•ä¼°**äºŒåˆ†é¡åˆ¤æ–·**çš„æº–ç¢ºæ€§

---

### 4ï¸âƒ£ Position Consistency (ä½ç½®ä¸€è‡´æ€§)

**å•é¡Œ**ï¼šã€Œæˆå°æ¯”è¼ƒæœ‰æ²’æœ‰ä½ç½®åè¦‹ï¼Ÿã€

```python
from boring.judge.metrics import pairwise_metrics

comparisons = [
    {"winner": "A", "position_consistent": True},
    {"winner": "B", "position_consistent": True},
    {"winner": "A", "position_consistent": False},  # ä½ç½®ä¸ä¸€è‡´
]

metrics = pairwise_metrics(comparisons)
print(f"Position Consistency: {metrics.position_consistency:.0%}")  # 67%
```

**ç”¨é€”**ï¼šæª¢æ¸¬**ä½ç½®åè¦‹**ï¼ˆç¬¬ä¸€å€‹é¸é …è¢«åå¥½çš„å‚¾å‘ï¼‰

---

## ğŸ“ˆ ä½•æ™‚ä½¿ç”¨å“ªå€‹æŒ‡æ¨™ï¼Ÿ

| ä½ çš„è©•ä¼°ä»»å‹™ | æ¨è–¦æŒ‡æ¨™ |
|--------------|----------|
| çµ¦ç¨‹å¼ç¢¼æ‰“ 1-5 åˆ† | **Kappa** + **Spearman** |
| åˆ¤æ–·ç¨‹å¼ç¢¼ å¥½/å£ | **F1 Score** |
| æ¯”è¼ƒå…©æ®µç¨‹å¼ç¢¼èª°æ›´å¥½ | **Position Consistency** |
| æª¢æŸ¥ AI è©•åˆ†æœ‰æ²’æœ‰åè¦‹ | **Bias Report** |

---

## ğŸ”§ MCP å·¥å…·ä½¿ç”¨

### æŸ¥çœ‹è©•ä¼°æŒ‡æ¨™

```
boring_evaluation_metrics
```

### æŸ¥çœ‹åè¦‹å ±å‘Š

```
boring_bias_report
```

### è‡ªç„¶èªè¨€è§¸ç™¼

```
boring "show evaluation metrics"
boring "è©•ä¼°æŒ‡æ¨™"
boring "show me the bias report"
boring "æŸ¥çœ‹åè¦‹å ±å‘Š"
```

---

## ğŸ“š é€²éšè³‡æº

- [LLM-as-a-Judge è«–æ–‡](https://arxiv.org/abs/2306.05685)
- [Cohen's Kappa è©³è§£](https://en.wikipedia.org/wiki/Cohen%27s_kappa)
- [Spearman ç›¸é—œä¿‚æ•¸](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)
